{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c558938f",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) æºç å®ç°\n",
    "\n",
    "æœ¬ Notebook ä»é›¶å®ç° Vision Transformerï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å’Œæµ‹è¯•ã€‚\n",
    "\n",
    "## å®ç°å†…å®¹\n",
    "\n",
    "### Part 1: åŸºç¡€æ¨¡å—\n",
    "- Patch Embedding æ¨¡å—\n",
    "- Position Embedding æ¨¡å—\n",
    "- å•å…ƒæµ‹è¯•éªŒè¯\n",
    "\n",
    "### Part 2: æ ¸å¿ƒç»„ä»¶\n",
    "- Multi-Head Self-Attention æœºåˆ¶\n",
    "- Transformer Encoder Block\n",
    "- å®Œæ•´ ViT æ¨¡å‹\n",
    "- å‰å‘ä¼ æ’­æµ‹è¯•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a704a2",
   "metadata": {},
   "source": [
    "## Part 1: åŸºç¡€æ¨¡å—å®ç°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0232f",
   "metadata": {},
   "source": [
    "### 1.1 å¯¼å…¥ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc5efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ç‰ˆæœ¬: 2.9.1+cu130\n",
      "CUDA å¯ç”¨: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c4382",
   "metadata": {},
   "source": [
    "### 1.2 å®ç° Patch Embedding\n",
    "\n",
    "Patch Embedding å°†å›¾åƒåˆ‡åˆ†æˆå›ºå®šå¤§å°çš„ patchesï¼Œå¹¶é€šè¿‡å·ç§¯å±‚æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ã€‚\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼š\n",
    "- ä½¿ç”¨ kernel_size=patch_size, stride=patch_size çš„å·ç§¯æ¥åˆ‡åˆ†å›¾åƒ\n",
    "- å°† 2D ç‰¹å¾å›¾å±•å¹³ä¸ºåºåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8fd167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PatchEmbed å®ç°å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"å›¾åƒåˆ°PatchåµŒå…¥çš„è½¬æ¢\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # ä½¿ç”¨å·ç§¯å®ç°patchåˆ‡åˆ†å’Œçº¿æ€§æŠ•å½±\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)            # (B, embed_dim, H/ps, W/ps)\n",
    "        x = x.flatten(2)            # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)       # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "print(\"âœ… PatchEmbed å®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b4aa7",
   "metadata": {},
   "source": [
    "### 1.3 æµ‹è¯• Patch Embedding\n",
    "\n",
    "éªŒè¯è¾“å‡ºç»´åº¦æ˜¯å¦æ­£ç¡®ï¼š\n",
    "- è¾“å…¥ï¼š(B, 3, 224, 224)\n",
    "- è¾“å‡ºï¼š(B, 196, 768) - 196 = (224/16)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af5a201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 3, 224, 224])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 196, 768])\n",
      "é¢„æœŸå½¢çŠ¶: (2, 196, 768)\n",
      "âœ… Patch Embedding æµ‹è¯•é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯• Patch Embedding\n",
    "def test_patch_embed():\n",
    "    x = torch.randn(2, 3, 224, 224)\n",
    "    pe = PatchEmbed()\n",
    "    out = pe(x)\n",
    "    \n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n",
    "    print(f\"é¢„æœŸå½¢çŠ¶: (2, 196, 768)\")\n",
    "    \n",
    "    assert out.shape == (2, 196, 768), f\"ç»´åº¦é”™è¯¯: {out.shape}\"\n",
    "    print(\"âœ… Patch Embedding æµ‹è¯•é€šè¿‡\")\n",
    "    return out\n",
    "\n",
    "patch_output = test_patch_embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059add3",
   "metadata": {},
   "source": [
    "### 1.4 å®ç° Position Embedding\n",
    "\n",
    "Position Embedding ä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯ï¼š\n",
    "- CLS token: ç”¨äºåˆ†ç±»çš„ç‰¹æ®Štoken\n",
    "- Position embeddings: å¯å­¦ä¹ çš„ä½ç½®ç¼–ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66943af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ViTEmbeddings å®ç°å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class ViTEmbeddings(nn.Module):\n",
    "    \"\"\"ViTçš„åµŒå…¥å±‚ï¼šæ·»åŠ CLS tokenå’Œä½ç½®ç¼–ç \"\"\"\n",
    "    def __init__(self, num_patches, embed_dim):\n",
    "        super().__init__()\n",
    "        # CLS token: ç”¨äºåˆ†ç±»çš„ç‰¹æ®Štoken\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # ä½ç½®ç¼–ç : num_patches + 1 (åŒ…å«CLS token)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, D) - Næ˜¯patchæ•°é‡\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # æ‰©å±•CLS tokenåˆ°batch\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        \n",
    "        # æ‹¼æ¥CLS tokenåˆ°åºåˆ—å¼€å¤´\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # æ·»åŠ ä½ç½®ç¼–ç \n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "print(\"âœ… ViTEmbeddings å®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccb112",
   "metadata": {},
   "source": [
    "### 1.5 æµ‹è¯• Position Embedding\n",
    "\n",
    "éªŒè¯ CLS token å’Œä½ç½®ç¼–ç æ˜¯å¦æ­£ç¡®æ·»åŠ ï¼š\n",
    "- è¾“å…¥ï¼š(B, 196, 768)\n",
    "- è¾“å‡ºï¼š(B, 197, 768) - å¤šäº†1ä¸ªCLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970039ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 196, 768])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 197, 768])\n",
      "é¢„æœŸå½¢çŠ¶: (2, 197, 768) - å¤šäº†1ä¸ªCLS token\n",
      "âœ… Position Embedding æµ‹è¯•é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯• Position Embedding\n",
    "def test_pos_embed():\n",
    "    x = torch.randn(2, 196, 768)\n",
    "    emb = ViTEmbeddings(num_patches=196, embed_dim=768)\n",
    "    out = emb(x)\n",
    "    \n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n",
    "    print(f\"é¢„æœŸå½¢çŠ¶: (2, 197, 768) - å¤šäº†1ä¸ªCLS token\")\n",
    "    \n",
    "    assert out.shape == (2, 197, 768), f\"ç»´åº¦é”™è¯¯: {out.shape}\"\n",
    "    print(\"âœ… Position Embedding æµ‹è¯•é€šè¿‡\")\n",
    "    return out\n",
    "\n",
    "pos_output = test_pos_embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f551e6",
   "metadata": {},
   "source": [
    "### 1.6 Part 1 æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6c3dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Part 1 å®Œæˆï¼\n",
      "==================================================\n",
      "âœ… Patch Embedding å®ç°å¹¶æµ‹è¯•é€šè¿‡\n",
      "âœ… Position Embedding å®ç°å¹¶æµ‹è¯•é€šè¿‡\n",
      "\n",
      "å‡†å¤‡è¿›å…¥ Part 2: å®ç°æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Part 1 å®Œæˆï¼\")\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… Patch Embedding å®ç°å¹¶æµ‹è¯•é€šè¿‡\")\n",
    "print(\"âœ… Position Embedding å®ç°å¹¶æµ‹è¯•é€šè¿‡\")\n",
    "print(\"\\nå‡†å¤‡è¿›å…¥ Part 2: å®ç°æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer...\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991356d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: æ ¸å¿ƒç»„ä»¶å®ç°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f003d",
   "metadata": {},
   "source": [
    "### 2.1 å®ç° Multi-Head Self-Attention\n",
    "\n",
    "å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒï¼š\n",
    "1. çº¿æ€§æŠ•å½±ç”Ÿæˆ Q, K, V\n",
    "2. åˆ†å‰²ä¸ºå¤šä¸ªå¤´\n",
    "3. è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "4. åˆå¹¶å¤šå¤´è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158662d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MultiHeadSelfAttention å®ç°å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # ä¸€æ¬¡æ€§ç”Ÿæˆ Q, K, V\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # 1. ç”Ÿæˆ Q, K, V\n",
    "        qkv = self.qkv(x)  # (B, N, 3*D)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆç¼©æ”¾ç‚¹ç§¯ï¼‰\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)  # (B, heads, N, N)\n",
    "\n",
    "        # 3. åº”ç”¨æ³¨æ„åŠ›åˆ° V\n",
    "        out = attn @ v  # (B, heads, N, head_dim)\n",
    "        \n",
    "        # 4. åˆå¹¶å¤šå¤´\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)\n",
    "        \n",
    "        # 5. è¾“å‡ºæŠ•å½±\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "print(\"âœ… MultiHeadSelfAttention å®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b1ea6",
   "metadata": {},
   "source": [
    "### 2.2 æµ‹è¯• Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2439b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 197, 768])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 197, 768])\n",
      "é¢„æœŸ: è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ\n",
      "âœ… Multi-Head Self-Attention æµ‹è¯•é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶\n",
    "def test_attention():\n",
    "    x = torch.randn(2, 197, 768)\n",
    "    attn = MultiHeadSelfAttention(embed_dim=768, num_heads=12)\n",
    "    out = attn(x)\n",
    "    \n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n",
    "    print(f\"é¢„æœŸ: è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ\")\n",
    "    \n",
    "    assert out.shape == x.shape, f\"ç»´åº¦é”™è¯¯: {out.shape}\"\n",
    "    print(\"âœ… Multi-Head Self-Attention æµ‹è¯•é€šè¿‡\")\n",
    "    return out\n",
    "\n",
    "attn_output = test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf3f1a",
   "metadata": {},
   "source": [
    "### 2.3 å®ç° Transformer Encoder Block\n",
    "\n",
    "Encoder Block ç»“æ„ï¼š\n",
    "1. LayerNorm + Multi-Head Attention + æ®‹å·®è¿æ¥\n",
    "2. LayerNorm + MLP + æ®‹å·®è¿æ¥\n",
    "\n",
    "ä½¿ç”¨ Pre-Norm æ¶æ„ï¼ˆæ›´ç¨³å®šï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbaab4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EncoderBlock å®ç°å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç¬¬ä¸€ä¸ªå­å±‚: å¤šå¤´è‡ªæ³¨æ„åŠ›\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "\n",
    "        # ç¬¬äºŒä¸ªå­å±‚: MLP (Feed-Forward)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-Norm æ¶æ„\n",
    "        # å­å±‚1: æ³¨æ„åŠ› + æ®‹å·®\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # å­å±‚2: MLP + æ®‹å·®\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "print(\"âœ… EncoderBlock å®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26fb31",
   "metadata": {},
   "source": [
    "### 2.4 æµ‹è¯• Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d51a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 197, 768])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 197, 768])\n",
      "é¢„æœŸ: è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ\n",
      "âœ… Encoder Block æµ‹è¯•é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯• Encoder Block\n",
    "def test_encoder_block():\n",
    "    x = torch.randn(2, 197, 768)\n",
    "    block = EncoderBlock(embed_dim=768, num_heads=12)\n",
    "    out = block(x)\n",
    "    \n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n",
    "    print(f\"é¢„æœŸ: è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ\")\n",
    "    \n",
    "    assert out.shape == x.shape, f\"ç»´åº¦é”™è¯¯: {out.shape}\"\n",
    "    print(\"âœ… Encoder Block æµ‹è¯•é€šè¿‡\")\n",
    "    return out\n",
    "\n",
    "block_output = test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807392fd",
   "metadata": {},
   "source": [
    "### 2.5 å®ç°å®Œæ•´çš„ Vision Transformer\n",
    "\n",
    "æ•´åˆæ‰€æœ‰æ¨¡å—æ„å»ºå®Œæ•´çš„ ViTï¼š\n",
    "1. Patch Embedding\n",
    "2. Position Embedding (å« CLS token)\n",
    "3. å¤šå±‚ Transformer Encoder\n",
    "4. åˆ†ç±»å¤´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e55440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VisionTransformer å®ç°å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"å®Œæ•´çš„ Vision Transformer æ¨¡å‹\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        num_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Patch Embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size, patch_size, in_chans, embed_dim\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # 2. Position Embedding (å« CLS token)\n",
    "        self.embeddings = ViTEmbeddings(num_patches, embed_dim)\n",
    "\n",
    "        # 3. Transformer Encoder (å¤šå±‚)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(embed_dim, num_heads)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # 4. åˆ†ç±»å¤´\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, C, H, W) -> (B, num_patches, embed_dim)\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # æ·»åŠ  CLS token å’Œä½ç½®ç¼–ç \n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # é€šè¿‡ Transformer Encoder\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # æœ€ç»ˆå½’ä¸€åŒ–\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # ä½¿ç”¨ CLS token è¿›è¡Œåˆ†ç±»\n",
    "        cls_token = x[:, 0]\n",
    "        return self.head(cls_token)\n",
    "\n",
    "print(\"âœ… VisionTransformer å®ç°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75ffa7",
   "metadata": {},
   "source": [
    "### 2.6 æµ‹è¯•å®Œæ•´æ¨¡å‹\n",
    "\n",
    "æµ‹è¯•æ ‡å‡† ViT-Base é…ç½®ï¼š\n",
    "- è¾“å…¥ï¼š224x224 RGB å›¾åƒ\n",
    "- Patch size: 16x16\n",
    "- Embed dim: 768\n",
    "- Depth: 12 å±‚\n",
    "- Heads: 12\n",
    "- Classes: 1000 (ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6c4091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "æµ‹è¯•å®Œæ•´ Vision Transformer æ¨¡å‹\n",
      "==================================================\n",
      "\n",
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 3, 224, 224])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 1000])\n",
      "é¢„æœŸå½¢çŠ¶: (2, 1000)\n",
      "\n",
      "âœ… å®Œæ•´æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼\n",
      "\n",
      "æ¨¡å‹å‚æ•°é‡: 86,567,656\n",
      "æ¨¡å‹å¤§å°: 330.23 MB (FP32)\n",
      "\n",
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 3, 224, 224])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 1000])\n",
      "é¢„æœŸå½¢çŠ¶: (2, 1000)\n",
      "\n",
      "âœ… å®Œæ•´æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼\n",
      "\n",
      "æ¨¡å‹å‚æ•°é‡: 86,567,656\n",
      "æ¨¡å‹å¤§å°: 330.23 MB (FP32)\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•å®Œæ•´ ViT æ¨¡å‹\n",
    "print(\"=\"*50)\n",
    "print(\"æµ‹è¯•å®Œæ•´ Vision Transformer æ¨¡å‹\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model = VisionTransformer()\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "y = model(x)\n",
    "\n",
    "print(f\"\\nè¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {y.shape}\")\n",
    "print(f\"é¢„æœŸå½¢çŠ¶: (2, 1000)\")\n",
    "\n",
    "if y.shape == (2, 1000):\n",
    "    print(\"\\nâœ… å®Œæ•´æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼\")\n",
    "else:\n",
    "    print(f\"\\nâŒ æµ‹è¯•å¤±è´¥: è¾“å‡ºå½¢çŠ¶ {y.shape} ä¸åŒ¹é…\")\n",
    "\n",
    "# ç»Ÿè®¡å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\næ¨¡å‹å‚æ•°é‡: {total_params:,}\")\n",
    "print(f\"æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24311d5",
   "metadata": {},
   "source": [
    "### 2.7 æµ‹è¯•ä¸åŒé…ç½®çš„ ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb61d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "æµ‹è¯• ViT-Small é…ç½® (CIFAR-10)\n",
      "==================================================\n",
      "è¾“å…¥å½¢çŠ¶: torch.Size([4, 3, 32, 32])\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([4, 10])\n",
      "é¢„æœŸå½¢çŠ¶: (4, 10)\n",
      "âœ… ViT-Small æµ‹è¯•é€šè¿‡\n",
      "\n",
      "æ¨¡å‹å‚æ•°é‡: 4,771,082\n",
      "æ¨¡å‹å¤§å°: 18.20 MB (FP32)\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯• ViT-Small (ç”¨äº CIFAR-10)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"æµ‹è¯• ViT-Small é…ç½® (CIFAR-10)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_small = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "x_small = torch.randn(4, 3, 32, 32)\n",
    "y_small = model_small(x_small)\n",
    "\n",
    "print(f\"è¾“å…¥å½¢çŠ¶: {x_small.shape}\")\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {y_small.shape}\")\n",
    "print(f\"é¢„æœŸå½¢çŠ¶: (4, 10)\")\n",
    "\n",
    "if y_small.shape == (4, 10):\n",
    "    print(\"âœ… ViT-Small æµ‹è¯•é€šè¿‡\")\n",
    "\n",
    "total_params_small = sum(p.numel() for p in model_small.parameters())\n",
    "print(f\"\\næ¨¡å‹å‚æ•°é‡: {total_params_small:,}\")\n",
    "print(f\"æ¨¡å‹å¤§å°: {total_params_small * 4 / 1024 / 1024:.2f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad02af5",
   "metadata": {},
   "source": [
    "### 2.8 å¯è§†åŒ–æ¨¡å‹ç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c8ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ViT-Base æ¨¡å‹ç»“æ„æ‘˜è¦\n",
      "==================================================\n",
      "\n",
      "é…ç½®:\n",
      "  - å›¾åƒå°ºå¯¸: 224x224\n",
      "  - Patch å°ºå¯¸: 16x16\n",
      "  - Patch æ•°é‡: 196\n",
      "  - åµŒå…¥ç»´åº¦: 768\n",
      "  - Transformer å±‚æ•°: 12\n",
      "  - æ³¨æ„åŠ›å¤´æ•°: 12\n",
      "  - MLP éšè—å±‚ç»´åº¦: 3072\n",
      "  - ç±»åˆ«æ•°: 1000\n",
      "\n",
      "ä¸»è¦ç»„ä»¶:\n",
      "  1. Patch Embedding (Conv2d)\n",
      "  2. CLS Token + Position Embedding\n",
      "  3. 12x Transformer Encoder Block\n",
      "     - Multi-Head Self-Attention (12 heads)\n",
      "     - MLP (768 -> 3072 -> 768)\n",
      "  4. Classification Head (Linear)\n",
      "\n",
      "æ€»å‚æ•°é‡: 86,567,656\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å°æ¨¡å‹ç»“æ„æ‘˜è¦\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ViT-Base æ¨¡å‹ç»“æ„æ‘˜è¦\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\né…ç½®:\")\n",
    "print(f\"  - å›¾åƒå°ºå¯¸: 224x224\")\n",
    "print(f\"  - Patch å°ºå¯¸: 16x16\")\n",
    "print(f\"  - Patch æ•°é‡: {(224//16)**2}\")\n",
    "print(f\"  - åµŒå…¥ç»´åº¦: 768\")\n",
    "print(f\"  - Transformer å±‚æ•°: 12\")\n",
    "print(f\"  - æ³¨æ„åŠ›å¤´æ•°: 12\")\n",
    "print(f\"  - MLP éšè—å±‚ç»´åº¦: {768*4}\")\n",
    "print(f\"  - ç±»åˆ«æ•°: 1000\")\n",
    "\n",
    "print(f\"\\nä¸»è¦ç»„ä»¶:\")\n",
    "print(f\"  1. Patch Embedding (Conv2d)\")\n",
    "print(f\"  2. CLS Token + Position Embedding\")\n",
    "print(f\"  3. 12x Transformer Encoder Block\")\n",
    "print(f\"     - Multi-Head Self-Attention (12 heads)\")\n",
    "print(f\"     - MLP (768 -> 3072 -> 768)\")\n",
    "print(f\"  4. Classification Head (Linear)\")\n",
    "\n",
    "print(f\"\\næ€»å‚æ•°é‡: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3a83e",
   "metadata": {},
   "source": [
    "### 2.9 Part 2 æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0553b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Part 2 å®Œæˆï¼\n",
      "==================================================\n",
      "âœ… Multi-Head Self-Attention å®ç°å¹¶æµ‹è¯•é€šè¿‡\n",
      "âœ… Transformer Encoder Block å®ç°å¹¶æµ‹è¯•é€šè¿‡\n",
      "âœ… å®Œæ•´ ViT æ¨¡å‹å®ç°å¹¶æµ‹è¯•é€šè¿‡\n",
      "âœ… æ¨¡å‹å¯æ­£å¸¸å‰å‘ä¼ æ’­\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Part 2 å®Œæˆï¼\")\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… Multi-Head Self-Attention å®ç°å¹¶æµ‹è¯•é€šè¿‡\")\n",
    "print(\"âœ… Transformer Encoder Block å®ç°å¹¶æµ‹è¯•é€šè¿‡\")\n",
    "print(\"âœ… å®Œæ•´ ViT æ¨¡å‹å®ç°å¹¶æµ‹è¯•é€šè¿‡\")\n",
    "print(\"âœ… æ¨¡å‹å¯æ­£å¸¸å‰å‘ä¼ æ’­\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b00b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ€ç»ˆæ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "622c02fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "               Vision Transformer å®ç°å®Œæˆï¼\n",
      "============================================================\n",
      "\n",
      "âœ… Part 1: åŸºç¡€æ¨¡å—\n",
      "   - Patch Embedding\n",
      "   - Position Embedding\n",
      "\n",
      "âœ… Part 2: æ ¸å¿ƒç»„ä»¶\n",
      "   - Multi-Head Self-Attention\n",
      "   - Transformer Encoder Block\n",
      "   - å®Œæ•´ ViT æ¨¡å‹\n",
      "\n",
      "âœ… æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡\n",
      "âœ… æ¨¡å‹å¯æ­£å¸¸å‰å‘ä¼ æ’­\n",
      "\n",
      "ğŸ“ å®ç°çš„æ¨¡å‹é…ç½®:\n",
      "   - ViT-Base: 86,567,656 å‚æ•°\n",
      "   - ViT-Small: 4,771,082 å‚æ•°\n",
      "\n",
      "ğŸ‰ ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒäº†ï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" \"*15 + \"Vision Transformer å®ç°å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ… Part 1: åŸºç¡€æ¨¡å—\")\n",
    "print(\"   - Patch Embedding\")\n",
    "print(\"   - Position Embedding\")\n",
    "print(\"\\nâœ… Part 2: æ ¸å¿ƒç»„ä»¶\")\n",
    "print(\"   - Multi-Head Self-Attention\")\n",
    "print(\"   - Transformer Encoder Block\")\n",
    "print(\"   - å®Œæ•´ ViT æ¨¡å‹\")\n",
    "print(\"\\nâœ… æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡\")\n",
    "print(\"âœ… æ¨¡å‹å¯æ­£å¸¸å‰å‘ä¼ æ’­\")\n",
    "print(\"\\nğŸ“ å®ç°çš„æ¨¡å‹é…ç½®:\")\n",
    "print(f\"   - ViT-Base: {total_params:,} å‚æ•°\")\n",
    "print(f\"   - ViT-Small: {total_params_small:,} å‚æ•°\")\n",
    "print(\"\\nğŸ‰ ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒäº†ï¼\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

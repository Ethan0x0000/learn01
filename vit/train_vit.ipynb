{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56377226",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) 训练 - CIFAR-10\n",
    "\n",
    "本 Notebook 实现了 ViT 在 CIFAR-10 数据集上的完整训练流程。\n",
    "\n",
    "## 训练配置\n",
    "- 配置文件：configs/vit_base.yaml\n",
    "- 数据集：CIFAR-10 (32x32 彩色图像，10分类)\n",
    "- 模型：Vision Transformer\n",
    "- 输出结构：logs/、runs/、checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24010d",
   "metadata": {},
   "source": [
    "## 1. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9729419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import platform\n",
    "import socket\n",
    "import subprocess\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from vit import VisionTransformer\n",
    "\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 设备: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3783f6",
   "metadata": {},
   "source": [
    "## 2. 超参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path('configs/vit_base.yaml')\n",
    "\n",
    "def _parse_scalar(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, (int, float, bool)):\n",
    "        return value\n",
    "    text = str(value).strip()\n",
    "    if text.lower() in {'true', 'false'}:\n",
    "        return text.lower() == 'true'\n",
    "    if text.lower() in {'null', 'none', '~'}:\n",
    "        return None\n",
    "    try:\n",
    "        if re.fullmatch(r'[+-]?\\d+', text):\n",
    "            return int(text)\n",
    "        if re.fullmatch(r'[+-]?(\\d+\\.\\d*|\\d*\\.\\d+)([eE][+-]?\\d+)?', text) or re.fullmatch(r'[+-]?\\d+[eE][+-]?\\d+', text):\n",
    "            return float(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if (text.startswith('\"') and text.endswith('\"')) or (text.startswith(\"'\") and text.endswith(\"'\")):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_yaml(path: Path):\n",
    "    try:\n",
    "        import yaml as _yaml\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            return _yaml.safe_load(f)\n",
    "    except Exception:\n",
    "        lines = []\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for raw in f:\n",
    "                if '#' in raw:\n",
    "                    raw = raw.split('#', 1)[0]\n",
    "                if raw.strip() == '':\n",
    "                    continue\n",
    "                lines.append(raw.rstrip('\\n'))\n",
    "\n",
    "        root = {}\n",
    "        stack = [(0, root)]\n",
    "\n",
    "        def current_container(indent):\n",
    "            while stack and indent < stack[-1][0]:\n",
    "                stack.pop()\n",
    "            return stack[-1][1]\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            indent = len(line) - len(line.lstrip(' '))\n",
    "            stripped = line.strip()\n",
    "            container = current_container(indent)\n",
    "\n",
    "            if stripped.startswith('- '):\n",
    "                item_text = stripped[2:].strip()\n",
    "                if not isinstance(container, list):\n",
    "                    raise ValueError('YAML parser only supports lists under explicit keys')\n",
    "                container.append(_parse_scalar(item_text))\n",
    "                continue\n",
    "\n",
    "            if ':' not in stripped:\n",
    "                raise ValueError(f'Invalid YAML line: {line}')\n",
    "            key, rest = stripped.split(':', 1)\n",
    "            key = key.strip()\n",
    "            rest = rest.strip()\n",
    "\n",
    "            if rest == '':\n",
    "                next_container = {}\n",
    "                for j in range(idx + 1, len(lines)):\n",
    "                    nxt = lines[j]\n",
    "                    nxt_indent = len(nxt) - len(nxt.lstrip(' '))\n",
    "                    if nxt.strip() == '':\n",
    "                        continue\n",
    "                    if nxt_indent <= indent:\n",
    "                        break\n",
    "                    if nxt.strip().startswith('- '):\n",
    "                        next_container = []\n",
    "                    break\n",
    "                container[key] = next_container\n",
    "                stack.append((indent + 2, next_container))\n",
    "            else:\n",
    "                if rest.startswith('[') and rest.endswith(']'):\n",
    "                    inner = rest[1:-1].strip()\n",
    "                    if inner == '':\n",
    "                        container[key] = []\n",
    "                    else:\n",
    "                        parts = [p.strip() for p in inner.split(',')]\n",
    "                        container[key] = [_parse_scalar(p) for p in parts]\n",
    "                else:\n",
    "                    container[key] = _parse_scalar(rest)\n",
    "\n",
    "        return root\n",
    "\n",
    "\n",
    "def dump_yaml(data, path: Path):\n",
    "    try:\n",
    "        import yaml as _yaml\n",
    "        with path.open('w', encoding='utf-8') as f:\n",
    "            _yaml.safe_dump(data, f, sort_keys=False, allow_unicode=True)\n",
    "        return\n",
    "    except Exception:\n",
    "        def _dump(obj, indent=0):\n",
    "            spaces = ' ' * indent\n",
    "            if isinstance(obj, dict):\n",
    "                out = []\n",
    "                for k, v in obj.items():\n",
    "                    if isinstance(v, (dict, list)):\n",
    "                        out.append(f'{spaces}{k}:')\n",
    "                        out.extend(_dump(v, indent + 2))\n",
    "                    else:\n",
    "                        if isinstance(v, bool):\n",
    "                            vs = 'true' if v else 'false'\n",
    "                        elif v is None:\n",
    "                            vs = 'null'\n",
    "                        else:\n",
    "                            vs = str(v)\n",
    "                        out.append(f'{spaces}{k}: {vs}')\n",
    "                return out\n",
    "            if isinstance(obj, list):\n",
    "                out = []\n",
    "                for item in obj:\n",
    "                    if isinstance(item, (dict, list)):\n",
    "                        out.append(f'{spaces}-')\n",
    "                        out.extend(_dump(item, indent + 2))\n",
    "                    else:\n",
    "                        if isinstance(item, bool):\n",
    "                            vs = 'true' if item else 'false'\n",
    "                        elif item is None:\n",
    "                            vs = 'null'\n",
    "                        else:\n",
    "                            vs = str(item)\n",
    "                        out.append(f'{spaces}- {vs}')\n",
    "                return out\n",
    "            return [f'{spaces}{str(obj)}']\n",
    "\n",
    "        text = '\\n'.join(_dump(data)) + '\\n'\n",
    "        path.write_text(text, encoding='utf-8')\n",
    "\n",
    "\n",
    "def get_next_exp_id(logs_root: Path):\n",
    "    logs_root.mkdir(parents=True, exist_ok=True)\n",
    "    max_id = 0\n",
    "    for p in logs_root.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = re.fullmatch(r'exp_(\\d+)', p.name)\n",
    "        if m:\n",
    "            max_id = max(max_id, int(m.group(1)))\n",
    "    return f'exp_{max_id + 1:03d}'\n",
    "\n",
    "\n",
    "cfg = load_yaml(CONFIG_PATH)\n",
    "EXP_ID = get_next_exp_id(Path('logs'))\n",
    "LOG_DIR = Path('logs') / EXP_ID\n",
    "RUN_DIR = Path('runs') / EXP_ID\n",
    "CKPT_DIR = Path('checkpoints') / EXP_ID\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dump_yaml(cfg, LOG_DIR / 'config.yaml')\n",
    "\n",
    "env = {\n",
    "    'timestamp': __import__('datetime').datetime.now().isoformat(),\n",
    "    'python_version': sys.version.split(' ')[0],\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_available': torch.cuda.is_available(),\n",
    "    'cuda_version': getattr(torch.version, 'cuda', None),\n",
    "    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "    'hostname': socket.gethostname(),\n",
    "    'platform': platform.platform(),\n",
    "}\n",
    "(LOG_DIR / 'env.json').write_text(json.dumps(env, indent=2, ensure_ascii=False) + '\\n', encoding='utf-8')\n",
    "\n",
    "def _try_git(cmd):\n",
    "    try:\n",
    "        return subprocess.check_output(['git'] + cmd, text=True, stderr=subprocess.STDOUT).strip()\n",
    "    except Exception as e:\n",
    "        return f'Unavailable: {e}'\n",
    "\n",
    "git_text = '\\n'.join([\n",
    "    f\"Commit: {_try_git(['rev-parse', 'HEAD'])}\",\n",
    "    f\"Branch: {_try_git(['rev-parse', '--abbrev-ref', 'HEAD'])}\",\n",
    "    '',\n",
    "    'Status:',\n",
    "    _try_git(['status', '--porcelain']),\n",
    "]) + '\\n'\n",
    "(LOG_DIR / 'git.txt').write_text(git_text, encoding='utf-8')\n",
    "\n",
    "seed = int(cfg.get('experiment', {}).get('seed', 42))\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_cfg = cfg.get('training', {})\n",
    "model_cfg = cfg.get('model', {})\n",
    "data_cfg = cfg.get('data', {})\n",
    "aug_cfg = cfg.get('augmentation', {})\n",
    "logging_cfg = cfg.get('logging', {})\n",
    "\n",
    "BATCH_SIZE = int(training_cfg.get('batch_size', 32))\n",
    "LEARNING_RATE = float(training_cfg.get('learning_rate', 1e-4))\n",
    "WEIGHT_DECAY = float(training_cfg.get('weight_decay', 0.05))\n",
    "EPOCHS = int(training_cfg.get('epochs', 10))\n",
    "NUM_WORKERS = int(training_cfg.get('num_workers', 2))\n",
    "PRINT_FREQ = int(training_cfg.get('print_freq', 100))\n",
    "SAVE_FREQ = int(training_cfg.get('save_freq', 1))\n",
    "\n",
    "IMG_SIZE = int(model_cfg.get('img_size', 32))\n",
    "PATCH_SIZE = int(model_cfg.get('patch_size', 4))\n",
    "NUM_CLASSES = int(model_cfg.get('num_classes', 10))\n",
    "EMBED_DIM = int(model_cfg.get('embed_dim', 256))\n",
    "DEPTH = int(model_cfg.get('depth', 6))\n",
    "NUM_HEADS = int(model_cfg.get('num_heads', 8))\n",
    "\n",
    "DATA_DIR = str(data_cfg.get('data_dir', './data'))\n",
    "NORMALIZE_MEAN = tuple(data_cfg.get('normalize_mean', [0.4914, 0.4822, 0.4465]))\n",
    "NORMALIZE_STD = tuple(data_cfg.get('normalize_std', [0.2023, 0.1994, 0.2010]))\n",
    "\n",
    "RANDOM_CROP = bool(aug_cfg.get('random_crop', True))\n",
    "CROP_PADDING = int(aug_cfg.get('crop_padding', 4))\n",
    "RANDOM_HORIZONTAL_FLIP = bool(aug_cfg.get('random_horizontal_flip', True))\n",
    "\n",
    "TENSORBOARD_ENABLED = bool(logging_cfg.get('tensorboard', True))\n",
    "LOG_INTERVAL = int(logging_cfg.get('log_interval', 10))\n",
    "SAVE_LOGS = bool(logging_cfg.get('save_logs', True))\n",
    "\n",
    "TRAIN_JSONL_PATH = LOG_DIR / 'train.jsonl'\n",
    "VAL_JSONL_PATH = LOG_DIR / 'val.jsonl'\n",
    "BEST_CKPT_PATH = CKPT_DIR / 'best_model.pth'\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"训练配置\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"实验ID: {EXP_ID}\")\n",
    "print(f\"配置文件: {CONFIG_PATH}\")\n",
    "print(f\"日志目录: {LOG_DIR}\")\n",
    "print(f\"TensorBoard: {RUN_DIR}\")\n",
    "print(f\"Checkpoint: {CKPT_DIR}\")\n",
    "print(f\"设备: {DEVICE}\")\n",
    "print(f\"批次大小: {BATCH_SIZE}\")\n",
    "print(f\"学习率: {LEARNING_RATE}\")\n",
    "print(f\"训练轮数: {EPOCHS}\")\n",
    "print(f\"模型配置: Embed={EMBED_DIM}, Depth={DEPTH}, Heads={NUM_HEADS}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a8774",
   "metadata": {},
   "source": [
    "## 3. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611951ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集数据增强\n",
    "train_transforms = []\n",
    "if RANDOM_CROP:\n",
    "    train_transforms.append(transforms.RandomCrop(IMG_SIZE, padding=CROP_PADDING))\n",
    "if RANDOM_HORIZONTAL_FLIP:\n",
    "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transforms.extend([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "])\n",
    "transform_train = transforms.Compose(train_transforms)\n",
    "\n",
    "# 测试集预处理\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "print(\"正在加载 CIFAR-10 数据集...\")\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=DATA_DIR, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=DATA_DIR, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"✅ 数据加载完成\")\n",
    "print(f\"训练集: {len(train_dataset)} 样本, {len(train_loader)} 批次\")\n",
    "print(f\"测试集: {len(test_dataset)} 样本, {len(test_loader)} 批次\")\n",
    "\n",
    "# CIFAR-10 类别名称\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f6ffd",
   "metadata": {},
   "source": [
    "## 4. 可视化样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示一些训练样本\n",
    "def imshow(img, title=None):\n",
    "    img = img / 2 + 0.5  # 反归一化\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# 获取一个批次的数据\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 显示前8张图片\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < 8:\n",
    "        ax.imshow(np.transpose(images[idx].numpy() / 2 + 0.5, (1, 2, 0)))\n",
    "        ax.set_title(classes[labels[idx]])\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"样本形状: {images.shape}\")\n",
    "print(f\"标签形状: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710dead",
   "metadata": {},
   "source": [
    "## 5. 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "model = VisionTransformer(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=3,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    depth=DEPTH,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_classes=NUM_CLASSES,\n",
    ").to(DEVICE)\n",
    "\n",
    "# 统计参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"模型信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"总参数量: {total_params:,}\")\n",
    "print(f\"可训练参数量: {trainable_params:,}\")\n",
    "print(f\"模型大小: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 测试前向传播\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"\\n✅ 模型前向传播测试通过\")\n",
    "    print(f\"输入形状: {test_input.shape}\")\n",
    "    print(f\"输出形状: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b15fe7",
   "metadata": {},
   "source": [
    "## 6. 设置训练组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec754dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer_cfg = cfg.get('optimizer', {})\n",
    "optimizer_type = str(optimizer_cfg.get('type', 'AdamW'))\n",
    "if optimizer_type.lower() != 'adamw':\n",
    "    raise ValueError(f'Unsupported optimizer: {optimizer_type}')\n",
    "betas = tuple(optimizer_cfg.get('betas', (0.9, 0.999)))\n",
    "eps = float(optimizer_cfg.get('eps', 1e-8))\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=betas,\n",
    "    eps=eps,\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler_cfg = cfg.get('scheduler', {})\n",
    "scheduler_type = str(scheduler_cfg.get('type', 'CosineAnnealing'))\n",
    "if scheduler_type.lower() not in {'cosineannealing', 'cosineannealinglr', 'cosine'}:\n",
    "    raise ValueError(f'Unsupported scheduler: {scheduler_type}')\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=int(scheduler_cfg.get('T_max', EPOCHS)),\n",
    ")\n",
    "\n",
    "print(\"✅ 训练组件初始化完成\")\n",
    "print(f\"优化器: {optimizer_type}\")\n",
    "print(f\"学习率调度: {scheduler_type}\")\n",
    "print(f\"损失函数: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d185e5c",
   "metadata": {},
   "source": [
    "## 7. 定义训练和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, history):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        \n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # 打印进度\n",
    "        if (batch_idx + 1) % PRINT_FREQ == 0:\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            acc = 100. * correct / total\n",
    "            print(f'  [{epoch+1}/{EPOCHS}] [{batch_idx+1}/{len(train_loader)}] '\n",
    "                  f'Loss: {avg_loss:.4f} Acc: {acc:.2f}%')\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_time\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"绘制训练历史\"\"\"\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss曲线\n",
    "    ax1.plot(epochs_range, history['train_loss'], 'b-o', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(epochs_range, history['test_loss'], 'r-s', label='Test Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy曲线\n",
    "    ax2.plot(epochs_range, history['train_acc'], 'b-o', label='Train Acc', linewidth=2)\n",
    "    ax2.plot(epochs_range, history['test_acc'], 'r-s', label='Test Acc', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ 训练函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987c98a",
   "metadata": {},
   "source": [
    "## 8. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525734f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"开始训练\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_acc = 0.0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "def append_jsonl(path: Path, record: dict):\n",
    "    with path.open('a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "writer = None\n",
    "if TENSORBOARD_ENABLED:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(str(RUN_DIR))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 训练\n",
    "    train_loss, train_acc, epoch_time = train_one_epoch(epoch, history)\n",
    "    \n",
    "    # 测试\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # 记录历史\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # 打印总结\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} 完成 (用时: {epoch_time:.1f}s)\")\n",
    "    print(f\"学习率: {current_lr:.6f}\")\n",
    "    print(f\"训练 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"测试 - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    if SAVE_LOGS:\n",
    "        append_jsonl(TRAIN_JSONL_PATH, {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': train_loss,\n",
    "            'acc': train_acc,\n",
    "            'lr': current_lr,\n",
    "            'time_sec': epoch_time,\n",
    "        })\n",
    "        append_jsonl(VAL_JSONL_PATH, {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': test_loss,\n",
    "            'acc': test_acc,\n",
    "        })\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('loss/train', train_loss, epoch + 1)\n",
    "        writer.add_scalar('loss/val', test_loss, epoch + 1)\n",
    "        writer.add_scalar('acc/train', train_acc, epoch + 1)\n",
    "        writer.add_scalar('acc/val', test_acc, epoch + 1)\n",
    "        writer.add_scalar('lr', current_lr, epoch + 1)\n",
    "        writer.flush()\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_acc': test_acc,\n",
    "            'history': history,\n",
    "        }, str(BEST_CKPT_PATH))\n",
    "        print(f\"✅ 保存最佳模型 (准确率: {test_acc:.2f}%)\")\n",
    "\n",
    "    if (epoch + 1) % SAVE_FREQ == 0 or (epoch + 1) == EPOCHS:\n",
    "        ckpt_path = CKPT_DIR / f'epoch_{epoch + 1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_acc': test_acc,\n",
    "            'history': history,\n",
    "        }, str(ckpt_path))\n",
    "        print(f\"✅ 保存Checkpoint: {ckpt_path.name}\")\n",
    "    \n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 实时绘制训练曲线\n",
    "    if epoch > 0:  # 至少有2个点才绘制\n",
    "        clear_output(wait=True)\n",
    "        plot_history(history)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"训练完成！\")\n",
    "print(f\"最佳测试准确率: {best_acc:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592ff82",
   "metadata": {},
   "source": [
    "## 9. 可视化训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制最终训练曲线\n",
    "plot_history(history)\n",
    "\n",
    "# 打印训练历史表格\n",
    "print(\"\\n训练历史详情:\")\n",
    "print(f\"{'Epoch':<8}{'Train Loss':<12}{'Train Acc':<12}{'Test Loss':<12}{'Test Acc':<12}{'LR':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(len(history['train_loss'])):\n",
    "    print(f\"{i+1:<8}{history['train_loss'][i]:<12.4f}{history['train_acc'][i]:<12.2f}\"\n",
    "          f\"{history['test_loss'][i]:<12.4f}{history['test_acc'][i]:<12.2f}{history['lr'][i]:<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92366ce",
   "metadata": {},
   "source": [
    "## 10. 测试模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ccb68",
   "metadata": {},
   "source": [
    "## 11. 分析每个类别的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "checkpoint = torch.load(str(BEST_CKPT_PATH), map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ 加载最佳模型 (Epoch {checkpoint['epoch']}, 准确率: {checkpoint['test_acc']:.2f}%)\")\n",
    "\n",
    "# 获取测试样本\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# 可视化预测结果\n",
    "images_np = images.cpu()\n",
    "predicted_np = predicted.cpu()\n",
    "labels_np = labels.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < 8:\n",
    "        img = images_np[idx].numpy() / 2 + 0.5\n",
    "        ax.imshow(np.transpose(img, (1, 2, 0)))\n",
    "        \n",
    "        pred_label = classes[predicted_np[idx]]\n",
    "        true_label = classes[labels_np[idx]]\n",
    "        color = 'green' if predicted_np[idx] == labels_np[idx] else 'red'\n",
    "        \n",
    "        ax.set_title(f'预测: {pred_label}\\n真实: {true_label}', color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 计算准确率\n",
    "correct = (predicted_np == labels_np).sum().item()\n",
    "print(f\"\\n这批样本的准确率: {100. * correct / len(labels):.2f}% ({correct}/{len(labels)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "checkpoint = torch.load(str(BEST_CKPT_PATH), map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ 加载最佳模型 (Epoch {checkpoint['epoch']}, 准确率: {checkpoint['test_acc']:.2f}%)\")\n",
    "\n",
    "# 获取测试样本\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# 可视化预测结果\n",
    "images_np = images.cpu()\n",
    "predicted_np = predicted.cpu()\n",
    "labels_np = labels.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < 8:\n",
    "        img = images_np[idx].numpy() / 2 + 0.5\n",
    "        ax.imshow(np.transpose(img, (1, 2, 0)))\n",
    "        \n",
    "        pred_label = classes[predicted_np[idx]]\n",
    "        true_label = classes[labels_np[idx]]\n",
    "        color = 'green' if predicted_np[idx] == labels_np[idx] else 'red'\n",
    "        \n",
    "        ax.set_title(f'预测: {pred_label}\\n真实: {true_label}', color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 计算准确率\n",
    "correct = (predicted_np == labels_np).sum().item()\n",
    "print(f\"\\n这批样本的准确率: {100. * correct / len(labels):.2f}% ({correct}/{len(labels)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d373ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "checkpoint = torch.load(str(BEST_CKPT_PATH), map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ 加载最佳模型 (Epoch {checkpoint['epoch']}, 准确率: {checkpoint['test_acc']:.2f}%)\")\n",
    "\n",
    "# 获取测试样本\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# 可视化预测结果\n",
    "images_np = images.cpu()\n",
    "predicted_np = predicted.cpu()\n",
    "labels_np = labels.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < 8:\n",
    "        img = images_np[idx].numpy() / 2 + 0.5\n",
    "        ax.imshow(np.transpose(img, (1, 2, 0)))\n",
    "        \n",
    "        pred_label = classes[predicted_np[idx]]\n",
    "        true_label = classes[labels_np[idx]]\n",
    "        color = 'green' if predicted_np[idx] == labels_np[idx] else 'red'\n",
    "        \n",
    "        ax.set_title(f'预测: {pred_label}\\n真实: {true_label}', color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 计算准确率\n",
    "correct = (predicted_np == labels_np).sum().item()\n",
    "print(f\"\\n这批样本的准确率: {100. * correct / len(labels):.2f}% ({correct}/{len(labels)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89364deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在整个测试集上评估\n",
    "class_correct = [0] * NUM_CLASSES\n",
    "class_total = [0] * NUM_CLASSES\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        c = (predicted == targets)\n",
    "        for i in range(len(targets)):\n",
    "            label = targets[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# 打印每个类别的准确率\n",
    "print(\"各类别准确率:\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(NUM_CLASSES):\n",
    "    acc = 100 * class_correct[i] / class_total[i]\n",
    "    print(f\"{classes[i]:<12}: {acc:>6.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "\n",
    "# 绘制柱状图\n",
    "class_acc = [100 * class_correct[i] / class_total[i] for i in range(NUM_CLASSES)]\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(classes, class_acc, color='steelblue', alpha=0.8)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Per-Class Accuracy on CIFAR-10 Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 100])\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 在柱子上标注数值\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58bf23",
   "metadata": {},
   "source": [
    "## 12. 保存训练摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练摘要\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'embed_dim': EMBED_DIM,\n",
    "        'depth': DEPTH,\n",
    "        'num_heads': NUM_HEADS,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_test_acc': best_acc,\n",
    "        'final_train_acc': history['train_acc'][-1],\n",
    "        'final_test_acc': history['test_acc'][-1],\n",
    "    },\n",
    "    'class_accuracy': {classes[i]: class_acc[i] for i in range(NUM_CLASSES)},\n",
    "    'history': history\n",
    "}\n",
    "\n",
    "with open(LOG_DIR / 'training_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 训练摘要已保存到 {LOG_DIR / 'training_summary.json'}\")\n",
    "print(\"\\n训练总结:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"训练轮数: {EPOCHS}\")\n",
    "print(f\"最佳测试准确率: {best_acc:.2f}%\")\n",
    "print(f\"最终训练准确率: {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"最终测试准确率: {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"模型参数量: {total_params:,}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
